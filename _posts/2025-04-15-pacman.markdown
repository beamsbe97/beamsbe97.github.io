---
layout: post
title: "Monte carlo tree search for multi-agent Pacman Capture the Flag"
thumbnail: /assets/images/pacman.png
date: "2025-04-15"
show_header: false
---



This project develops a vanilla MCTS agent and progressively augments it with heuristics to better handle the dynamics of the Capture the Flag environment in Pacman.


<h3>Abstract</h3>
Monte Carlo Tree Search (MCTS) has demonstrated strong performance in deterministic and adversarial games. However, its vanilla form lacks context-awareness in complex multi-agent environments.

We explore integrating heuristic knowledge into MCTS for the UC Berkeley Capture the Flag variant of Pacman, introducing:

- Progressive bias via heuristic-based action sorting
- Softmax-weighted rollouts for more accurate simulations
- A specialized defensive MCTS agent for team coordination

Through ablation studies and a tournament (evaluated with win rates and TrueSkill scores), we show that heuristic-informed strategies significantly improve robustness and gameplay effectiveness.

---

<h3>Introduction</h3>

Monte Carlo Tree Search (MCTS) is widely used in domains with large branching factors where defining a good evaluation function is infeasible. Unlike Minimax, which explores uniformly, MCTS balances exploration and exploitation using policies such as UCB1.

We apply MCTS to the UC Berkeley Capture the Flag variant of Pacman, a challenging multi-agent adversarial environment where two teams of agents must steal food while defending their own territory.

<img src="/assets/images/pacman.png"
     alt="Pacman environment"
     class="post-image"
     style="width: 600px; display: block; margin: 2.5rem auto;">

<p class="image-caption">
  Figure1: Pacman Capture-the-Flag environment
</p>

---

<h3>Background</h3>
Background

MCTS operates in four stages:

- Selection – Traverses the tree using UCB1: [ UCB1 = \bar{X_j} + 2C_p \sqrt{\frac{\ln n}{n_j}} ] balancing exploitation and exploration.
- Expansion – Adds a new child node for an unexplored action.
- Simulation – Runs rollouts (random or heuristic-driven) to estimate outcomes.
- Backpropagation – Updates visit counts and rewards along the path.

To adapt MCTS to Pacman CTF, we added:

- Progressive Bias – guiding expansion with heuristics
- Softmax Rollouts – probabilistic selection favoring promising states

---

<h3>Methods</h3>
Baselines

- Heuristic Reflex Agent: Offensive agent collects food; defensive agent intercepts opponents.
- Vanilla MCTS: Uniform random rollouts with default hyperparameters:
    - Rollout depth: 10
    - Simulation epsilon (ε): 0.2
    - Exploration constant (Cp): 0.707

Domain Adaptations

- Perfect Information assumption (access to full game state)
- Fixed time per move (0.5s budget)
- Limited rollout depth
- Removing STOP action from legal actions
- Custom state evaluation with features:
    - Distance to food (+1.5)
    - Carrying pellets (+20.0)
    - Distance to home (+2.0)
    - Ghost proximity (−8.0)
- ε-greedy simulation policy
- Early return-to-base rule when carrying food

Heuristic Enhancements

- Untried Action Sorting: Chooses best successor via heuristic evaluation
- Softmax Weighted Rollouts:
[ P(a_i) = \frac{e^{s_i/\tau}}{\sum_j e^{s_j/\tau}} ]
biases simulations toward high-value states
- Rainbow Agent: Combines weighted rollouts, RAVE, and action sorting
- Defensive Agent:
    - Pursues visible intruders
    - Patrols territory when no intruders detected


---
<h3>Experiments</h3>


- Setup: UC Berkeley Pacman AI framework
- Evaluation: 100 games per configuration, multiple seeds
- Metrics: win rate, food collected, oscillation rate, decision time
- Tournament: Round-robin (100 matches per agent pair) + TrueSkill ranking

Hyperparameter Ablation

- Rollout Depth: Too shallow misses opportunities, too deep causes loops; best = 10
- Simulation Epsilon (ε): Best trade-off at 0.2–0.3
- Cp (exploration constant): Best at 0.707

---
<h3>Results</h3>

<img src="/assets/images/pacman_matrix.png"
     alt="Pacman environment"
     class="post-image"
     style="width: 600px; display: block; margin: 2.5rem auto;">

<p class="image-caption">
  Confusion matrix of head-to-head matches.
</p>

| Agent Variant | TrueSkill Score | Win % |
| :--- | :--- | :--- |
| MCTS Sorting Rollout | **35.60** | 65.38% |
| MCTS Weighted Rollout | 35.29 | 67.03% |
| MCTS w/ Heuristic Eval | 35.06 | 58.47% |
| Rainbow MCTS | 33.55 | 58.43% |
| RAVE MCTS | 31.17 | 40.22% |
| Baseline Heuristic Agent | 25.97 | 25.97% |
| Vanilla MCTS | **7.11** | 0.00% |

---
<h3>Analysis</h3>

- Rollout depth = 10 avoids noise and loops
- ε = 0.2–0.3 balances exploitation and exploration
- Cp = 0.707 consistent with literature best practice
- Heuristic-guided rollouts improve decision quality significantly
- Action sorting accelerates convergence to good policies
- Defensive agent increases stability in multi-agent play
- Vanilla MCTS fails completely, highlighting need for heuristics

---
<h3>Future Work</h3>

We propose extending MCTS with Deep Reinforcement Learning:

- Integrating DQN / Double DQN for function approximation
- Using experience replay and target network updates for stability
- Learning from raw game features instead of hand-crafted heuristics

Such integration could surpass heuristic-driven methods and enable adaptive learning in real-time adversarial environments.
