---
layout: post
title: "Generating Playable Mario Levels from Agent Experience"
thumbnail: /assets/images/mario.png
date: "2025-05-10"
show_header: false
---

<video width="100%" controls>
  <source src="/assets/videos/mario_gen_playthrough.webm" type="video/mp4">
</video>
<h2>
Generating Playable Mario Levels from Agent Experience
</h2>


This project explores the intersection of procedural content generation (PCG) and reinforcement learning (RL) for creating playable Super Mario Bros levels. By combining generative models with trained RL agents, we enable the automatic design of levels with controllable difficulty.

<h3>Abstract</h3>

We propose a dual pipeline framework where generative models (DCGAN and diffusion models) are trained to generate Mario levels conditioned on empirically derived difficulty scores. These scores are computed from RL agents (Dueling DQN and PPO) trained to play Mario levels.

- Levels are processed into symbolic representations.
- Generators output new level segments conditioned on difficulty.
- RL agents evaluate playability, enabling iterative feedback.

Key findings:

- DCGANs produced the most coherent and playable levels.
- Diffusion models underperformed, struggling with structural coherence.
- PPO agents achieved stable performance and successfully navigated generated levels.

This approach demonstrates a scalable method for difficulty-aware procedural content generation.

---

<h3>Introduction</h3>

- Generative AI (GenAI) creates new content by learning patterns in data (e.g., GANs, diffusion models).
- Reinforcement Learning (RL) trains agents to learn decision-making by interacting with environments.

Both have been applied to video games, which offer controlled environments for AI research. Prior work (Volz et al. 2018) used DCGANs + evolutionary algorithms to evolve Mario levels.

Our contribution: Replace scripted evolutionary search with learning agents (DQN, PPO) to evaluate difficulty, and integrate conditional GANs & diffusion models for controllable level generation.

---

<h3>Methodology</h3>
The pipeline consists of two main stages:

- Agent Training & Difficulty Scoring
    - RL agents play training levels until proficient.
    - A quality score is computed from completion, retries, movements, and rewards.
    - Levels are labeled as Easy, Medium, Hard.
- Generative Models for Level Design
    - Models trained on symbolic Mario levels.
    - Conditioned on difficulty labels.
    - Output new playable levels.
    
<img src="/assets/images/mario_pipeline.png"
     alt="mario_pipeline"
     class="post-image"
     style="width: 600px; display: block;">

---

Levels Processing
- Levels are stored as symbolic text (characters → tiles).
- Converted into identity matrices → one-hot/dense embeddings.
- Models trained on abstract representations, decoded back into symbolic → PNGs.

<img src="/assets/images/mario_levelprocessing.png"
     alt="mario_pipeline"
     class="post-image"
     style="width: 600px; display: block; align:left;">

---
<h3>Level Generation Models</h3>

<h4>Baseline MLP</h4>

- Simple fully-connected generator.
- Fails to preserve spatial structure.
- Output: unplayable, mostly floor tiles.

<img src="/assets/images/mario_baseline.png"
     alt="belief_distorted"
     class="post-image"
     style="width: 600px; display: block;">

<h4>DCGAN</h4>
- Uses transposed convolutions for spatial preservation.
- Trained with adversarial setup.
- Required hyperparameter search (learning rates, batch size, latent dimension).
- Generated coherent Mario-like levels.

<img src="/assets/images/mario_levels.png"
     alt="belief_distorted"
     class="post-image"
     style="width: 600px; display: block;">

---

<h3>RL Agents</h3>
Dueling DQN
- Splits Q-value into Value stream and Advantage stream.
- Improves stability and sample efficiency.

PPO (Proximal Policy Optimization)
- Actor-Critic with clipped surrogate objective.
- Stable and efficient for sparse reward settings.
- Outperformed DQN in final evaluations.


---
<h3>Experiments</h3>

- Difficulty Classification
    - Levels scored & labeled.
    - Example: Easy (>0), Medium (-25–0), Hard (<-25).
- DCGAN Ablation Study
    - Tested learning rates, batch sizes, noise std, smoothing.
    - Selected best configuration for stable generation.
- Diffusion Training
    - Progressive architecture refinements (V1–V4).
    - Strong convergence, weak level coherence.
- Agent Evaluation on Generated Levels
    - PPO tested on generated levels.
    - Mixed results: some levels aligned with difficulty labels, others mismatched.

---
<h3>Results</h3>
<h4>MLP</h4>
- Failed to generate realistic levels.

<h4>DCGAN</h4>
- Produced playable, coherent levels.
- PPO successfully completed most generated levels.
- Some difficulty mismatches occurred.

<img src="/assets/images/mario_dcgan_results.png"
     alt="belief_distorted"
     class="post-image"
     style="width: 600px; display: block;">


<h4>Diffusion Model</h4>
- Training loss converged.
- Generated outputs lacked structure.
- Requires architectural improvements (e.g., attention).

<img src="/assets/images/diffusion_loss.png"
     alt="belief_distorted"
     class="post-image"
     style="width: 600px; display: block;">


| Generated Level | Intended Difficulty | Agent Score / Observed Difficulty |
| :--- | :--- | :--- |
| level 1.txt | Easy | 1080.64 (Easy ✅) |
| level 2.txt | Medium | -46.19 (Hard ❌) |
| level 3.txt | Hard | -79.44 (Hard ✅) |
| level 4.txt | Easy | -4.12 (Medium ❌) |
| level 5.txt | Medium | 5.07 (Easy ❌) |

---
<h3>Conclusion</h3>

- DCGANs were most effective, generating playable levels.
- PPO agents validated playability and completed generated levels.
- Diffusion models struggled without attention mechanisms.
- Conditional generation is feasible: agent-based difficulty labels guided content creation

---
<h3>Future Work</h3>

- Improved Diffusion Models: integrate self-attention for spatial dependencies, use one-hot outputs for symbolic consistency.
- Smarter Agents: semantic embeddings of tiles, adaptive PPO variants.
- Generalization: expand dataset, generate larger full levels instead of patches.
